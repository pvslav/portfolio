{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef64c80d-f07e-4dff-8812-46cec4624059",
   "metadata": {},
   "source": [
    "# Pandas fundamentals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edc067-8992-40e0-ac89-480df23bb9ef",
   "metadata": {},
   "source": [
    "## Filtering and ordering\n",
    "\n",
    "Filtering, or conditional selection, produces a Series of True/False booleans based on the specified condition of each record.\n",
    "\n",
    "Examples:\n",
    "\n",
    "`df[df['column_name'] <= 100]`\n",
    "\n",
    "`df.loc[df.column_name <= 100]`\n",
    "\n",
    "\n",
    "#### Combining conditions using AND / OR logic\n",
    "\n",
    "We can use the ampersand `(&)` to bring the two conditions together and apply `'AND'` logic, or use `(|)` to apply `'OR'` logic.\n",
    "\n",
    "Example:\n",
    "\n",
    "`df.loc[(df.year>2016) & (df.brand == 'Honda')]`\n",
    "\n",
    "`df.loc[(df.year>2016) | (df.brand == 'Honda')]`\n",
    "\n",
    "\n",
    "#### Using `isin` to check if the data is in the list of values\n",
    "\n",
    "We can use `isin`\n",
    "\n",
    "`df.loc[df.brand.isin(['Honda','Nissan','Toyota'])]`\n",
    "\n",
    "\n",
    "#### Using `isnull()` and `notnull()` to find empty and non-empty data\n",
    "\n",
    "`df.loc[df.price.isnull()]`\n",
    "\n",
    "`df.loc[df.price.notnull()]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676c9c9-6761-418d-be96-d6bcb2e887ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING\n",
    "\n",
    "# Filter by column value\n",
    "df[df['column_name'] <= 100]\n",
    "\n",
    "# Filter by checking if column values are in the list\n",
    "specific_countries = ['Bangladesh', 'Brazil']\n",
    "df[df['Country'].isin(specific_countries)]\n",
    "\n",
    "# Filter by checking if column values contains string fragment\n",
    "df[df['Country'].str.contains('United')]\n",
    "\n",
    "# Set the DataFrame index using existing column\n",
    "df2 = df.set_index('Country')\n",
    "\n",
    "## Filtering by column names. axis = 1 - indicates we search in column names\n",
    "df2.filter(items=['Continent','CCA3'], axis = 1)\n",
    "\n",
    "## Filter by column names using 'like'\n",
    "df2.filter(like = 'Pop', axis = 1) # lists all columns which names contain 'Pop'\n",
    "\n",
    "## Filtering by row names (index). axis = 0 - indicates we search in row names\n",
    "df2.filter(items=['Zimbabwe'], axis = 0)\n",
    "\n",
    "## Filter by row names using 'like'\n",
    "df2.filter(like = 'United', axis = 0)\n",
    "\n",
    "## Filter by row name\n",
    "df2.loc['Zambia']\n",
    "\n",
    "## Filter by row position (integer)\n",
    "df2.iloc[51]\n",
    "\n",
    "\n",
    "# SORTING / ORDERING\n",
    "\n",
    "# Sorting by column descending\n",
    "df[df['Rank'] < 10].sort_values(by='Rank', ascending=False)\n",
    "\n",
    "# Sorting by a few columns ascending\n",
    "df[df['Rank'] < 10].sort_values(by=['Continent','Country'], ascending=True)\n",
    "\n",
    "# Sorting by a few columns applying different ascending option to each sorted column\n",
    "df[df['Rank'] < 10].sort_values(by=['Continent','Country'], ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c7a071-bf6b-4f9e-849e-417016e6ed66",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Index is an object that stores the access labels for all Pandas objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862416c-3611-4c65-9b68-c0cfd3b4b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying custom index\n",
    "\n",
    "# Set the DataFrame index using existing column\n",
    "df = df.set_index('Country')\n",
    "\n",
    "## Set the DataFrame index using existing column without saving df to a variable using 'inplace = True'\n",
    "## if use w/o inplace=True, changes won't save\n",
    "df.set_index('Country', inplace = True)\n",
    "\n",
    "# Alternative way to set custom index while reading file\n",
    "df = pd.read_csv(\"world_population.csv\", index_col = \"Country\")\n",
    "\n",
    "# Importing dataset to pandas dataframe with Date column, making index to be the Date column, and parse each index value as Date\n",
    "pd.read_csv(\"path_to_dataset_file\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Reset the index. inplace=True means modifying existing dataframe instead of creating a new one\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Setting multi-index. In the example below, two columns 'Continent' and 'Country' will be the index\n",
    "df.set_index(['Continent', 'Country'], inplace=True)\n",
    "\n",
    "# Sort index\n",
    "df.sort_index()\n",
    "df.sort_index(ascending=False) # in descending order\n",
    "df.sort_index(ascending=[False, True]) # specify different sorting order for different indexes\n",
    "\n",
    "# Accessing elements using loc and iloc in multi-indexed dataframe\n",
    "df.loc['Africa', 'Angola'] # searching for 'Africa' as a continent and 'Angola' as a country\n",
    "df.iloc[0] # even in case of multi-indexed dataframe will be lookup rows using initial integer-based index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b8485-7a7f-4826-b702-f1cf1fceaeb9",
   "metadata": {},
   "source": [
    "## Indexing, Selecting & Assigning\n",
    "\n",
    "Two valid ways of accessing fields in Pandas dataframe:\n",
    " - `df.column_name`. For example, `pd.country`. It won't work if there spaces and other symbols in column name. In this regard, using [] approach is safer.\n",
    " - `df['column_name']`. For example, `pd['country']`\n",
    "\n",
    "Pandas Dataframe represents the whole dataset, whereas pandas Series represents a column of it.\n",
    "\n",
    "Indexing operator `[]` can be used to access single specific value of a column,\n",
    "for example \n",
    "`reviews['country'][0]`\n",
    "\n",
    "\n",
    "### Indexing in Pandas\n",
    "Using pandas accessor operators, `loc` and `iloc`.\n",
    "Both `loc` and `iloc` are row-first, column-second.\n",
    "\n",
    "`loc` and `iloc` use slightly different indexing schemes:\n",
    " - `iloc` - the first element of the range is included and the last one excluded. So `0:10` will select entries `0,...,9`\n",
    " - `loc` - the first element of the range is included and the last one is included. So `0:10` will select entries `0,...,10`\n",
    "\n",
    "Otherwise, the semantics of using `loc` are the same as those for `iloc`.\n",
    "\n",
    "Two paradigms for attribute selection:\n",
    " - index-based selection\n",
    " - label-based selection\n",
    "\n",
    "\n",
    "#### index-based selection\n",
    "Selecting data based on its numerical position in the data. `iloc` follows this paradigm.\n",
    "\n",
    "When we use `iloc` we treat the dataset like a big matrix (a list of lists), one that we have to index into by position.\n",
    "\n",
    "`df.iloc[row_index, column_index]` - general format\n",
    "\n",
    "`df.iloc[0]` - selects the first row of data in a DataFrame\n",
    "\n",
    "`df.iloc[:, 0]` - selects the first column of data in a DataFrame\n",
    "\n",
    "`df.iloc[:3, 0]` - selects the first three rows from the first column in a DataFrame\n",
    "\n",
    "`df.iloc[1:3, 2]` - selects the second and third entries from the third column in a DataFrame\n",
    "\n",
    "It's also possible to pass a list:\n",
    "`df.iloc[[1,2,3,5,8]]` - selects rows with indexes 1, 2, 3, 5, and 8 \n",
    "`df.iloc[[5, 7, 11], 4]` - selects the 6th, 8th, and 12th rows from the 5-th column in a DataFrame\n",
    "`df.loc[[0,1,10,100], ['country','province','region_1','region_2']]` - selects rows with indexes 0, 1, 10, 100 from columns 'country','province','region_1','region_2'.\n",
    "\n",
    "Negative numbers can be used in selection. \n",
    "This will start counting forwards from the end of the values. \n",
    "\n",
    "`df.iloc[-5:]` - selects the last five elements of the dataset\n",
    "`df.iloc[-5:, 4]` - selects the last five elements from the 5-th column of the dataset\n",
    "\n",
    "\n",
    "#### Label-based selection\n",
    "The second paradigm for attribute selection. It uses the `loc` operator. In this paradigm, it's the data index value, not its position. So, `loc` uses the information in the indices to do its work.\n",
    "\n",
    "`df.loc[0, 'column_name']` - get the first entry from the column in data frame\n",
    "\n",
    "`df.loc[:4, ['col_name1', 'col_name2', 'col_name3']]` - gets the first four rows for specified column names\n",
    "\n",
    "\n",
    "\n",
    "### Assigning data\n",
    "\n",
    "#### Assigning a constant value\n",
    "\n",
    "`df['column_name'] = 'value'`\n",
    "Each row in the column will have this value\n",
    "\n",
    "#### Assigning an iterable of values\n",
    "\n",
    "`df['index_backwards'] = range(len(df), 0, -1)`\n",
    "\n",
    "\n",
    "### Other\n",
    "\n",
    "`idxmax()` - method used to find the index label of the first occurrence of the maximum value in a pandas Series or DataFrame. It returns the index label of the row that contains the maximum value.\n",
    "\n",
    "`max_index = df['column_name'].idxmax()`\n",
    "\n",
    "`df.loc[max_index, 'another_column_name']`\n",
    "This combination can be used to solve such tasks as \"Find the attribute of the dataset with the highest another attribute in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa5437-3422-4d1f-84c6-1b1b9844e2ca",
   "metadata": {},
   "source": [
    "## Summary Functions and Maps\n",
    "\n",
    "### Some summary functions\n",
    "\n",
    "`df.column_name.mean()` - calculates the mean for the column\n",
    "\n",
    "`df.column_name.median()` - calculates the median for the column\n",
    "\n",
    "`df.column_name.unique()` - shows a list of unique values in the column\n",
    "\n",
    "`df.column_name.nunique()` - shows the number of unique values in the column\n",
    "\n",
    "`df.column_name.value_counts()` - shows a list of unique values and how often they occur in the dataset\n",
    "\n",
    "\n",
    "### Maps\n",
    "\n",
    "Map is a function that takes one set of values and \"maps\" them to another set of values. \n",
    "\n",
    "There are two mapping methods:\n",
    " - `map()`\n",
    " - `apply()`\n",
    "\n",
    "`map()` and `apply()` return new, transformed Series and DataFrames, respectively. They don't modify the original data they're called on. \n",
    "\n",
    "#### Using `map()`\n",
    "For example, suppose that we wanted to remean the scores the wines received to 0. We can do this as follows:\n",
    "\n",
    "`review_points_mean = reviews.points.mean()`\n",
    "`reviews.points.map(lambda p: p - review_points_mean)`\n",
    "\n",
    "The function you pass to `map()` should expect a single value from the Series (a point value, in the above example), and return a transformed version of that value. \n",
    "`map()` returns a new Series where all the values have been transformed by your function.\n",
    "\n",
    "\n",
    "#### Using `apply()`\n",
    "`apply()` is the equivalent method if we want to transform a whole DataFrame by calling a custom method on each row.\n",
    "\n",
    "```\n",
    "def remean_points(row):\n",
    "    row.points = row.points - review_points_mean\n",
    "    return row\n",
    "\n",
    "reviews.apply(remean_points, axis='columns')\n",
    "```\n",
    "\n",
    "If we had called `reviews.apply()` with `axis='index'`, then instead of passing a function to transform each *row*, we would need to give a function to transform each *column*.\n",
    "\n",
    "\n",
    "#### Other\n",
    "Pandas provides many common mapping operations as built-ins. For example, here's a faster way of remeaning our points column:\n",
    "\n",
    "```\n",
    "review_points_mean = reviews.points.mean()\n",
    "reviews.points - review_points_mean\n",
    "```\n",
    "\n",
    "In this code we are performing an operation between a lot of values on the left-hand side (everything in the Series) and a single value on the right-hand side (the mean value).\n",
    "\n",
    "\n",
    "An easy way of combining `country` and `region` information in the dataset would be to do the following:\n",
    "\n",
    "`reviews.country + \" - \" + reviews.region_1`\n",
    "\n",
    "`df.brand + ' ' + df.model_name`\n",
    "\n",
    "`df.price * 2`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e91f6a-0048-42c1-844f-35151cf680e8",
   "metadata": {},
   "source": [
    "## Group by and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eba372-a5f4-463c-a837-6da82e09c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Base Flavor' field and calculate 'mean' for all integer fields in the dataframe\n",
    "df.groupby('Base Flavor').mean()\n",
    "\n",
    "# size() vs count()\n",
    "## size() method calculates the total number of records within each group, including both non-null and null values, \n",
    "## count() method calculates the number of non-null values within each group.\n",
    "### Group by 'Base Flavor' field and calculate 'count' for all fields in the dataframe\n",
    "df.groupby('Base Flavor').count()\n",
    "### Group by column_name and calculate the number of entries in each group\n",
    "df.groupby('column_name').size()\n",
    "\n",
    "# Group by column_name, calcuate count for each group and sort values in descending order\n",
    "df.groupby('column_name').size().sort_values(ascending=False)\n",
    "\n",
    "# Group by 'Base Flavor' field and calculate 'min' for all fields in the dataframe.\n",
    "## for string values it shows first value in the alphabetical order\n",
    "df.groupby('Base Flavor').min()\n",
    "\n",
    "# Group by 'Base Flavor' field and calculate 'max' for all fields in the dataframe.\n",
    "## for string values it shows last value in the alphabetical order\n",
    "df.groupby('Base Flavor').max()\n",
    "\n",
    "# Group by 'Base Flavor' field and calculate 'sum' for all integer fields in the dataframe\n",
    "df.groupby('Base Flavor').sum()\n",
    "\n",
    "# Group by 'Base Flavor' field and calculate 'mean' for the 'Flavor Rating' field\n",
    "df.groupby('Base Flavor')['Flavor Rating'].mean()\n",
    "\n",
    "# Using `agg()` method - allows to run a bunch of different functions on DataFrame simultaneously\n",
    "## for example, generate a simple statistical summary of the dataset as follows:\n",
    "df.groupby(['column_name']).column_name.agg([len, min, max])\n",
    "\n",
    "# General syntax of grouping and aggregating\n",
    "df.groupby('column_name').agg({'column_name': 'aggregation_function_name'})\n",
    "## for example.\n",
    "df.groupby('Base Flavor').agg({'Total Rating': 'max'})\n",
    "\n",
    "## Making several aggregation functions at once \n",
    "df.groupby('Base Flavor').agg({'Total Rating': ['mean','max','count','sum']})\n",
    "\n",
    "## Making several aggregation functions for several fields at once \n",
    "df.groupby('Base Flavor').agg({'Total Rating': ['mean','max','count','sum'], 'Flavor Rating': ['mean','max','count','sum']})\n",
    "\n",
    "## Grouping by several fields and making several aggregation functions for several fields at once\n",
    "df.groupby(['Base Flavor','Liked']).agg({'Total Rating': ['mean','max','count','sum'], 'Flavor Rating': ['mean','max','count','sum']})\n",
    "\n",
    "## Using describe() function to show general statistics for the grouped dataframe\n",
    "df.groupby(['Base Flavor']).describe()\n",
    "\n",
    "## Example: selecting the name of the first wine reviewed from each winery in the vines dataset\n",
    "df.groupby('winery').apply(lambda x: x.title.iloc[0])\n",
    "\n",
    "## Example: pick out the wine with max score by country and province\n",
    "df.groupby(['country', 'province']).apply(lambda x: x.loc[x.points.idxmax()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751033a-586c-4c09-8def-38b1ccb56b53",
   "metadata": {},
   "source": [
    "## Renaming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f5083-4373-4ae4-83c8-9400344aa828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dataframe column name\n",
    "df.rename(columns={'old_column_name': 'new_column_name'})\n",
    "\n",
    "# Rename two columns in dataframe at once\n",
    "df.rename(columns={'old_column1_name': 'new_column1_name', 'old_column2_name': 'new_column2_name'})\n",
    "df.rename(columns=dict(old_column1_name='new_column1_name', old_column2_name='new_column2_name')) \n",
    "\n",
    "# Rename first two entries of index\n",
    "df.rename(index={0: 'firstEntry', 1: 'secondEntry'})\n",
    "\n",
    "# Rename the name attribute for row index and the column index \n",
    "df.rename_axis(\"wines\", axis='rows').rename_axis(\"fields\", axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539add8-c360-4051-a750-cad6b2fce4f8",
   "metadata": {},
   "source": [
    "## Merge, Join, and Concatenate dataframes in Pandas\n",
    "\n",
    "![image.png](./image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddbd90-d54e-43b8-869c-6764a44a99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE #\n",
    "\n",
    "## The default join type in merge() is an inner join, which keeps only the rows with matching keys in both DataFrames.\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using inner join and one common field 'FellowshipID'\n",
    "df1.merge(df2, how = 'inner', on = 'FellowshipID')\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using inner join and two common fields, 'FellowshipID' and 'FirstName'\n",
    "df1.merge(df2, how = 'inner', on = ['FellowshipID', 'FirstName'])\n",
    "## the same result can be achieved by using `df1.merge(df2)` command. It's possible due to default values in the function\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using inner join and columns from both dataframes\n",
    "df1.merge(df2, how='inner', left_on='df1_column', right_on='df2_column')\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using outer join\n",
    "df1.merge(df2, how = 'outer')\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using left join\n",
    "df1.merge(df2, how = 'left')\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using right join\n",
    "df1.merge(df2, how = 'right')\n",
    "\n",
    "# Merging two dataframes, df1 and df2, using cross join that returns the Cartesian product of rows from the tables in the join. \n",
    "# In other words, it combines each row from the first table with each row from the second table\n",
    "df1.merge(df2, how = 'cross')\n",
    "\n",
    "\n",
    "# JOIN #\n",
    "## join() lets combine different DataFrame objects which have an index in common\n",
    "\n",
    "## The default join type in join() is a left join.\n",
    "## Functions the same as 'merge', but has a bit trickier syntax. So, it might be easier to use 'merge' for joins.\n",
    "\n",
    "# Joining two dataframes, df1 and df2. 'how' specifies type of join, 'inner', 'outer', 'left', 'right', 'cross'\n",
    "df3 = df1.set_index('FellowshipID').join(df2.set_index('FellowshipID'), lsuffix = '_Left', rsuffix = '_Right', how = 'inner')\n",
    "\n",
    "\n",
    "# CONCATENATE #\n",
    "\n",
    "## As opposed to 'join' and 'merge', 'concat' combines one dataframe on top of another, functioning similar to\n",
    "## 'UNION ALL' in SQL\n",
    "\n",
    "# Combining two dataframes, df1 and df2.\n",
    "pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d97aa-5b2d-4565-8ecd-4418e5f6177e",
   "metadata": {},
   "source": [
    "## Parsing Dates\n",
    "\n",
    "In most cases, parsing dates means converting from string representation to actual date format supported by pandas.\n",
    "\n",
    "The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are lots of possible parts of a date, but the most common are `%d` for day, `%m` for month, `%y` for a two-digit year and `%Y` for a four digit year.\n",
    "\n",
    "To parse dates, we should `import datetime`\n",
    "\n",
    "Some examples:\n",
    "\n",
    "* `1/17/07` has the format `\"%m/%d/%y\"`\n",
    "* `17-1-2007` has the format `\"%d-%m-%Y\"`\n",
    "\n",
    "```\n",
    "df['date_parsed'] = pd.to_datetime(df['date'], format=\"%m/%d/%y\")\n",
    "```\n",
    "\n",
    "What if I run into an error with multiple date formats? While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you can have pandas try to infer what the right date format should be. You can do that like so:\n",
    "```\n",
    "df['date_parsed'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
    "```\n",
    "\n",
    "Why don't you always use `infer_datetime_format = True`? \n",
    "There are two big reasons not to always have pandas guess the time format. The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates.\n",
    "\n",
    "\n",
    "Now that we have a column of parsed dates, we can extract information like the day of the month that a landslide occurred.\n",
    "\n",
    "```\n",
    "# get the day of the month from the date_parsed column\n",
    "day_of_month_df = df['date_parsed'].dt.day\n",
    "```\n",
    "\n",
    "Sometimes, it might happen that you mixed days with months in a date. One of the ways to double-check this, plot a histogram of the days of the month. We expect it to have values between 1 and 31\n",
    "```\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_df, kde=False, bins=31)\n",
    "```\n",
    "\n",
    "### Other tips\n",
    "\n",
    "Useful method to check length of date column in dataframe to ensure data consistency. \n",
    "```\n",
    "date_lengths = df.Date.str.len()\n",
    "date_lengths.value_counts()\n",
    "```\n",
    "\n",
    "If there are some inconsitent values with different lengths, we can locate them as follow:\n",
    "```\n",
    "indices = np.where([date_lengths == 24])[1]\n",
    "print('Indices with corrupted data:', indices)\n",
    "df.loc[indices]\n",
    "```\n",
    "Then to fix them:\n",
    "```\n",
    "df.loc[3378, \"Date\"] = \"02/23/1975\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841538d-977d-4962-8257-c23cbe064df1",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Three broad categories of chart types:\n",
    " - Trends\n",
    " - Relationship\n",
    " - Distribution\n",
    "\n",
    "**Trends** - A trend is defined as a pattern of change.\n",
    " - `sns.lineplot` - **Line charts** are best to show trends over a period of time, and multiple lines can be used to show trends in more than one group.\n",
    "\n",
    "\n",
    "**Relationship** - There are many different chart types that you can use to understand relationships between variables in your data.\n",
    " - `sns.barplot` - **Bar charts** are useful for comparing quantities corresponding to different groups.\n",
    " - `sns.heatmap` - **Heatmaps** can be used to find color-coded patterns in tables of numbers.\n",
    " - `sns.scatterplot` - **Scatter plots** show the relationship between two continuous variables; if color-coded, we can also show the relationship with a third categorical variable.\n",
    " - `sns.regplot` - Including a **regression line** in the scatter plot makes it easier to see any linear relationship between two variables.\n",
    " - `sns.lmplot` - This command is useful for drawing multiple regression lines, if the scatter plot contains multiple, color-coded groups.\n",
    " - `sns.swarmplot` - Categorical scatter plots show the relationship between a continuous variable and a categorical variable.\n",
    "\n",
    "**Distribution** - We visualize distributions to show the possible values that we can expect to see in a variable, along with how likely they are.\n",
    " - `sns.histplot` - **Histograms** show the distribution of a single numerical variable.\n",
    " - `sns.kdeplot` - **KDE plots** (or **2D KDE plots**) show an estimated, smooth distribution of a single numerical variable (or two numerical variables).\n",
    " - `sns.jointplot` - This command is useful for simultaneously displaying a 2D KDE plot with the corresponding KDE plots for each individual variable.\n",
    " \n",
    "### Change the style\n",
    "Seaborn has five different themes:\n",
    " - \"darkgrid\"\n",
    " - \"whitegrid\"\n",
    " - \"dark\"\n",
    " - \"white\"\n",
    " - \"ticks\"\n",
    "\n",
    "Change the style of the figure to the \"dark\" theme\n",
    "\n",
    "`sns.set_style(\"dark\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9087dc9-28c7-43a6-b124-00be425f6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Shows available plot design styles from plt module\n",
    "print(plt.style.available)\n",
    "\n",
    "# Applying selected style to plots\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Creating a plot. By default, line plot type is created\n",
    "df.plot()\n",
    "df.plot(kind = 'line') # the same\n",
    "\n",
    "# Creating line chart with seaborn\n",
    "## Set the width and height of the figure in inches\n",
    "plt.figure(figsize=(14,6))\n",
    "## Add title\n",
    "plt.title(\"Title for chart\")\n",
    "# Add label for horizontal axis\n",
    "plt.xlabel(\"X axis label\")\n",
    "## Create line chart\n",
    "sns.lineplot(data=dataframe)\n",
    "## Creating line chart for one selected column\n",
    "sns.lineplot(data=df['column_name'], label=\"label for chart\")\n",
    "\n",
    "# Creating a series of plots using 'subplots' parameter\n",
    "df.plot(kind = 'line', subplots = True)\n",
    "\n",
    "# Creating a plot with title\n",
    "df.plot(kind = 'line', title = 'Ice Cream Ratings')\n",
    "\n",
    "# Creating a line plot with title, xlabel, ylabel\n",
    "df.plot(kind = 'line', title = 'Ice Cream Ratings', xlabel = 'Date', ylabel = 'Scores')\n",
    "\n",
    "# CREATING BAR CHARTS\n",
    "# Creating bar chart\n",
    "df.plot(kind='bar')\n",
    "\n",
    "## Creating stacked bar chart\n",
    "df.plot(kind='bar', stacked = True)\n",
    "\n",
    "## Creating bar chart for one field 'Flavor Rating'\n",
    "df['Flavor Rating'].plot(kind='bar', stacked = True)\n",
    "\n",
    "## Creating horizontal stacked bar chart\n",
    "df.plot(kind='barh', stacked = True)\n",
    "\n",
    "## Creating bar chart with seaborn\n",
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(10,6))\n",
    "# Add title\n",
    "plt.title(\"Chart Title\")\n",
    "# Bar chart showing average arrival delay for Spirit Airlines flights by month\n",
    "sns.barplot(x=data_for_x, y=data_for_y)\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Label for y axis\")\n",
    "\n",
    "# Creating scatter plot\n",
    "## Requires two variables, 'x' and 'y'. s - size, c - color; optional parameters\n",
    "df.plot.scatter(x = 'Texture Rating', y = 'Overall Rating', s = 300, c = 'green')\n",
    "\n",
    "# Creating scatter plot with seaborn\n",
    "sns.scatterplot(x=df.column_1, y=df.column_2)\n",
    "## Double-check the strength of relationship by adding regression line, or the line that best fits the data.\n",
    "sns.regplot(x=df.column_1, y=df.column_2)\n",
    "## Color-coded scatter plots enable to display relationships between three variables\n",
    "sns.scatterplot(x=df.column_1, y=df.column_2, hue=df.column_3)\n",
    "## Add several regression lines for each group based on column_name3 values\n",
    "sns.lmplot(x=\"column_name1\", y=\"column_name2\", hue=\"column_name3\", data=dataframe)\n",
    "## Categorical scatter plot. To feature a categorical variable (like \"smoker\", Yes or No)\n",
    "sns.swarmplot(x=df.column_1, y=df.column_2)\n",
    "\n",
    "# Creating histogram\n",
    "## by default, bins = 10\n",
    "df.plot.hist(bins = 20)\n",
    "\n",
    "# Creating histogram with seaborn\n",
    "sns.histplot(df.column_name)\n",
    "\n",
    "# Creating boxplot\n",
    "df.plot.box()\n",
    "\n",
    "# Creating areaplot\n",
    "df.plot.area()\n",
    "df.plot.area(figsize = (10,4)) # changing size of the plot area to make it bigger\n",
    "\n",
    "# Creating pie chart\n",
    "df.plot.pie(y = 'Flavor Rating')\n",
    "df.plot.pie(y = 'Flavor Rating', figsize = (10,6))\n",
    "\n",
    "# Creating heatmap with seaborn\n",
    "## Set the width and height of the figure\n",
    "plt.figure(figsize=(14,7))\n",
    "## Add title\n",
    "plt.title(\"Heatmap Title\")\n",
    "## Heatmap. ## annot=True - to ensure that the values for each cell appear on the chart. \n",
    "## (Leaving this out removes the numbers from each of the cells!)\n",
    "sns.heatmap(data=heatmap_data, annot=True)\n",
    "## Add label for horizontal axis\n",
    "plt.xlabel(\"X label\")\n",
    "\n",
    "# Creating density plots with seaborn\n",
    "## Creating kernel density estimate (KDE). It's like a smoothed histogram.\n",
    "sns.kdeplot(data=df.column_name, fill=True) # fill=True colors the area below the curve\n",
    "## 2D KDE plots with 2 columns\n",
    "sns.jointplot(x=df.column_name_1, y=df.column_name_2, kind=\"kde\")\n",
    "## Color-coded plots\n",
    "# data= provides the name of the variable that we used to read in the data\n",
    "# x= sets the name of column with the data we want to plot\n",
    "# hue= sets the column we'll use to split the data into different histograms\n",
    "sns.histplot(data=df, x='column_name_1', hue='column_name_2')\n",
    "## Color-coded kde plots\n",
    "sns.kdeplot(data=df, x='column_name_1', hue='column_name_2', fill=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b0d4c2-3ab6-4840-8d1f-24744321fc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flavor Rating</th>\n",
       "      <th>Texture Rating</th>\n",
       "      <th>Overall Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/1/2022</th>\n",
       "      <td>0.223090</td>\n",
       "      <td>0.040220</td>\n",
       "      <td>0.600129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/2/2022</th>\n",
       "      <td>0.635886</td>\n",
       "      <td>0.938476</td>\n",
       "      <td>0.106264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/2022</th>\n",
       "      <td>0.442323</td>\n",
       "      <td>0.044154</td>\n",
       "      <td>0.598112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/4/2022</th>\n",
       "      <td>0.389128</td>\n",
       "      <td>0.549676</td>\n",
       "      <td>0.489353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/5/2022</th>\n",
       "      <td>0.386887</td>\n",
       "      <td>0.519439</td>\n",
       "      <td>0.988280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/6/2022</th>\n",
       "      <td>0.877984</td>\n",
       "      <td>0.193588</td>\n",
       "      <td>0.832827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/7/2022</th>\n",
       "      <td>0.140995</td>\n",
       "      <td>0.325110</td>\n",
       "      <td>0.105147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Flavor Rating  Texture Rating  Overall Rating\n",
       "Date                                                   \n",
       "1/1/2022       0.223090        0.040220        0.600129\n",
       "1/2/2022       0.635886        0.938476        0.106264\n",
       "1/3/2022       0.442323        0.044154        0.598112\n",
       "1/4/2022       0.389128        0.549676        0.489353\n",
       "1/5/2022       0.386887        0.519439        0.988280\n",
       "1/6/2022       0.877984        0.193588        0.832827\n",
       "1/7/2022       0.140995        0.325110        0.105147"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Ice Cream Ratings.csv')\n",
    "df = df.set_index('Date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f7621-8727-461f-abfd-e2a6ebb3796a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
